{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5665228-c1ae-4cde-9d43-cc595ada0896",
   "metadata": {},
   "source": [
    "# scaling the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca1c892-7e3f-4dd4-ae28-f6b166a5dbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data loaded successfully. ---\n",
      "Original dataset shape: (1030, 9)\n",
      "\n",
      "--- Data split into training, validation, and test sets. ---\n",
      "Training set:   721 samples (70%)\n",
      "Validation set: 154 samples (15%)\n",
      "Test set:       155 samples (15%)\n",
      "\n",
      "--- Feature scaling applied successfully. ---\n",
      "\n",
      "Preprocessing complete. Data is ready in the returned dictionary.\n",
      "\n",
      "--- Accessing the processed data ---\n",
      "\n",
      "Scaled training features (first 5 rows):\n",
      "     cement      slag    flyash     water  superplasticizer  coarseaggregate  \\\n",
      "0 -0.828484 -0.855296  0.761701 -0.766488          0.227273         0.415545   \n",
      "1  0.374823 -0.855296 -0.816913  0.103748         -1.013995         1.136979   \n",
      "2  0.317566  1.568935 -0.816913 -1.234713          1.352690        -1.551191   \n",
      "3  0.688809 -0.638541  1.397859 -1.315115          0.789982        -0.405309   \n",
      "4 -1.130465  1.312252  1.507813 -0.132729          2.130551        -1.730912   \n",
      "\n",
      "   fineaggregate       age  \n",
      "0       1.676803 -0.292980  \n",
      "1       0.141904 -0.633845  \n",
      "2       1.358333 -0.698772  \n",
      "3       0.366706 -0.698772  \n",
      "4      -0.382635 -0.292980  \n",
      "\n",
      "Training labels (first 5 rows):\n",
      "196    25.72\n",
      "631    17.54\n",
      "81     25.20\n",
      "526    23.64\n",
      "830    33.76\n",
      "Name: csMPa, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_data(file_path, target_column):\n",
    "    \"\"\"\n",
    "    Loads data from a CSV, splits it into train/validation/test sets,\n",
    "    and applies StandardScaler to the features.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        target_column (str): The name of the column to be used as the target/label.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the processed data splits\n",
    "              (X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "              and the fitted scaler object.\n",
    "    \"\"\"\n",
    "    # 1. Load the dataset from the CSV file\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "        return None\n",
    "\n",
    "    print(\"--- Data loaded successfully. ---\")\n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "    # 2. Separate features (X) and target (y)\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "\n",
    "    # 3. First split: 70% for training, 30% for temp (validation + test)\n",
    "    # The random_state ensures that the split is the same every time you run it.\n",
    "    # ðŸ’¡ FIX: Removed the 'stratify' parameter here.\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    # 4. Second split: Split the 30% temp set into 15% validation and 15% test\n",
    "    # ðŸ’¡ FIX: Removed the 'stratify' parameter here as well.\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Data split into training, validation, and test sets. ---\")\n",
    "    print(f\"Training set:   {X_train.shape[0]} samples (70%)\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples (15%)\")\n",
    "    print(f\"Test set:       {X_test.shape[0]} samples (15%)\")\n",
    "\n",
    "    # 5. Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "    X_val_scaled = pd.DataFrame(X_val_scaled, columns=X.columns)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "\n",
    "    print(\"\\n--- Feature scaling applied successfully. ---\")\n",
    "\n",
    "    # 6. Store all results in a single dictionary\n",
    "    preprocessed_data = {\n",
    "        'X_train': X_train_scaled,\n",
    "        'y_train': y_train,\n",
    "        'X_val': X_val_scaled,\n",
    "        'y_val': y_val,\n",
    "        'X_test': X_test_scaled,\n",
    "        'y_test': y_test,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "    \n",
    "    print(\"\\nPreprocessing complete. Data is ready in the returned dictionary.\")\n",
    "    \n",
    "    return preprocessed_data\n",
    "\n",
    "# --- EXAMPLE USAGE ---\n",
    "\n",
    "# 1. Specify your file path and the name of your target column\n",
    "csv_file = \"../../data analyzing/Concrete_Data_Yeh.csv\"\n",
    "target = 'csMPa'\n",
    "\n",
    "# 2. Call the function to get the processed data\n",
    "processed_results = preprocess_data(csv_file, target)\n",
    "\n",
    "# 3. Now you can easily access all the datasets from the 'processed_results' dictionary\n",
    "if processed_results:\n",
    "    print(\"\\n--- Accessing the processed data ---\")\n",
    "    print(\"\\nScaled training features (first 5 rows):\")\n",
    "    print(processed_results['X_train'].head())\n",
    "    \n",
    "    print(\"\\nTraining labels (first 5 rows):\")\n",
    "    print(processed_results['y_train'].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
